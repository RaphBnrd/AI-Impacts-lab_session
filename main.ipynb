{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6254286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from utils import experiment_classif_simple, format_results, \\\n",
    "    plot_loss_acc_over_epochs, plot_time_vs_parameters\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# device = 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f1a5b",
   "metadata": {},
   "source": [
    "## Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = 1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dec2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(10000000)):\n",
    "    _ = 1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bfbbdc",
   "metadata": {},
   "source": [
    "## Setup MNIST Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b5511",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ae02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Plot examples of digits (with true labels)\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "fig = plt.figure()\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow(example_data[i].cpu().squeeze(), cmap='gray')\n",
    "    plt.title(f\"True: {example_targets[i].item()}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f80a6",
   "metadata": {},
   "source": [
    "## Classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac19d8",
   "metadata": {},
   "source": [
    "### Expe - Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a8a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(torch.nn.Module):\n",
    "    def __init__(self, hidden_dims=[], input_dim=28*28, output_dim=10):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(torch.nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            prev_dim = h_dim\n",
    "        layers.append(torch.nn.Linear(prev_dim, output_dim))\n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a82e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims_tested = [\n",
    "    [32, 16], \n",
    "    [64, 32],\n",
    "    [128, 64, 32]\n",
    "]\n",
    "all_outputs_mlp = []\n",
    "all_models_mlp = []\n",
    "\n",
    "for hidden_dims in hidden_dims_tested:\n",
    "    print(f\"Training SimpleMLP with hidden dimensions: {hidden_dims}\")\n",
    "    model = SimpleMLP(hidden_dims=hidden_dims)\n",
    "    output = experiment_classif_simple(\n",
    "        SimpleMLP(hidden_dims=hidden_dims), \n",
    "        train_loader, test_loader,\n",
    "        nbr_epochs=10, device=device,\n",
    "        run_name=f\"SimpleMLP_{'_'.join(map(str, hidden_dims))}\"\n",
    "    )\n",
    "    all_outputs_mlp.append(output)\n",
    "    all_models_mlp.append(model)\n",
    "\n",
    "results_long_mlp, results_summary_mlp = format_results(all_outputs_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ae405",
   "metadata": {},
   "source": [
    "### Expe - Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(torch.nn.Module):\n",
    "    def __init__(self, input_channels=1, hidden_channels=[32, 64], output_dim=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        conv_part = []\n",
    "        in_channels = input_channels\n",
    "        for i in range(len(hidden_channels)):\n",
    "            conv_part.append(torch.nn.Conv2d(in_channels, hidden_channels[i], kernel_size=3, padding=1))\n",
    "            conv_part.append(torch.nn.ReLU())\n",
    "            in_channels = hidden_channels[i]\n",
    "        self.last_channels = hidden_channels[-1]\n",
    "        self.conv_part = torch.nn.Sequential(*conv_part)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        mlp_part = []\n",
    "        mlp_part.append(torch.nn.Linear(self.last_channels * 14 * 14, 128))\n",
    "        mlp_part.append(torch.nn.ReLU())\n",
    "        mlp_part.append(torch.nn.Dropout(0.25))\n",
    "        mlp_part.append(torch.nn.Linear(128, output_dim))\n",
    "        self.mlp_part = torch.nn.Sequential(*mlp_part)\n",
    "    def forward(self, x):\n",
    "        x = self.conv_part(x) # shape: (batch_size, last_channels, 28, 28)\n",
    "        x = self.pool(x)      # shape: (batch_size, last_channels, 14, 14)\n",
    "        x = x.view(-1, self.last_channels * 14 * 14) # flatten\n",
    "        x = self.mlp_part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels_tested = [\n",
    "    [8], \n",
    "    [16, 16]\n",
    "]\n",
    "all_outputs_cnn = []\n",
    "all_models_cnn = []\n",
    "\n",
    "for hidden_chans in hidden_channels_tested:\n",
    "    print(f\"Training SimpleCNN with hidden channels: {hidden_chans}\")\n",
    "    model = SimpleCNN(hidden_channels=hidden_chans)\n",
    "    output = experiment_classif_simple(\n",
    "        model, \n",
    "        train_loader, test_loader,\n",
    "        nbr_epochs=10, device=device,\n",
    "        run_name=f\"SimpleCNN_{'_'.join(map(str, hidden_chans))}\"\n",
    "    )\n",
    "    all_outputs_cnn.append(output)\n",
    "    all_models_cnn.append(model)\n",
    "\n",
    "results_long_cnn, results_summary_cnn = format_results(all_outputs_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3799a",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_summary = pd.concat([results_summary_mlp, results_summary_cnn], ignore_index=True)\n",
    "results_long = pd.concat([results_long_mlp, results_long_cnn], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_vs_parameters(results_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2427f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc_over_epochs(results_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de75a8",
   "metadata": {},
   "source": [
    "## Image generation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798f7f3",
   "metadata": {},
   "source": [
    "### CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb0493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE_MLP(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 28, 28), hidden_dims=[400],\n",
    "                 latent_dim=20, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = input_shape\n",
    "        self.input_dim = torch.prod(torch.tensor(input_shape)).item()\n",
    "        \n",
    "        # Encoder: image (784) + one hot label (10)\n",
    "        encoder = []\n",
    "        in_dim = self.input_dim + num_classes\n",
    "        for h in hidden_dims:\n",
    "            encoder.append(nn.Linear(in_dim, h))\n",
    "            encoder.append(nn.ReLU())\n",
    "            in_dim = h\n",
    "        self.encoder = nn.Sequential(*encoder)\n",
    "        self.fc_mu = nn.Linear(in_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(in_dim, latent_dim)\n",
    "\n",
    "        # Decoder: latent z + label (one hot)\n",
    "        decoder = []\n",
    "        in_dim = latent_dim + num_classes\n",
    "        for h in reversed(hidden_dims):\n",
    "            decoder.append(nn.Linear(in_dim, h))\n",
    "            decoder.append(nn.ReLU())\n",
    "            in_dim = h\n",
    "        self.decoder = nn.Sequential(*decoder)\n",
    "        self.fc_out = nn.Linear(in_dim, self.input_dim)\n",
    "\n",
    "    def encode(self, x, y_onehot):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        inp = torch.cat([x, y_onehot], dim=1)\n",
    "        h = F.relu(self.encoder(inp))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, y_onehot):\n",
    "        inp = torch.cat([z, y_onehot], dim=1)\n",
    "        h = F.relu(self.decoder(inp))\n",
    "        x_hat = torch.sigmoid(self.fc_out(h))\n",
    "        return x_hat.view(-1, *self.input_shape)\n",
    "\n",
    "    def forward(self, x, y_onehot):\n",
    "        mu, logvar = self.encode(x, y_onehot)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z, y_onehot)\n",
    "        return x_hat, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d51555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE_CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=1,\n",
    "        hidden_channels=[32, 64],\n",
    "        latent_dim=16,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        #  ENCODER: Conv + pooling + class conditioning\n",
    "        # -------------------------------------------------\n",
    "\n",
    "        # We inject class information by repeating a one-hot map\n",
    "        # of shape (B, num_classes, 28, 28)\n",
    "        in_channels = input_channels + num_classes\n",
    "        conv_blocks = []\n",
    "\n",
    "        for out_channels in hidden_channels:\n",
    "            conv_blocks.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "            conv_blocks.append(nn.ReLU())\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.encoder_conv = nn.Sequential(*conv_blocks)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # downsample to 14×14\n",
    "\n",
    "        # After pooling, size = last_channel × 14 × 14\n",
    "        flat_dim = hidden_channels[-1] * 14 * 14\n",
    "\n",
    "        self.fc_mu = nn.Linear(flat_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(flat_dim, latent_dim)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        #  DECODER: MLP → ConvTranspose pipeline + conditioning\n",
    "        # -------------------------------------------------\n",
    "\n",
    "        self.fc_decode = nn.Linear(latent_dim + num_classes, flat_dim)\n",
    "\n",
    "        decoder_channels = list(reversed(hidden_channels))\n",
    "\n",
    "        deconv_blocks = []\n",
    "        in_channels = decoder_channels[0]\n",
    "\n",
    "        for i, out_channels in enumerate(decoder_channels[1:] + [input_channels]):\n",
    "\n",
    "            # --- KEY POINT ---\n",
    "            # Only the *last* hidden layer should perform upsampling:\n",
    "            # 14x14 → 28x28\n",
    "            if i == 0:\n",
    "                # 1st deconv: keep size (14→14)\n",
    "                deconv_blocks.append(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            elif i == len(decoder_channels) - 1:\n",
    "                # FINAL deconv: upsample 14x14 → 28x28\n",
    "                deconv_blocks.append(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=4,\n",
    "                        stride=2,\n",
    "                        padding=1,          # clean ×2 upsampling\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                # Optional intermediate blocks (kept at 14×14)\n",
    "                deconv_blocks.append(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # add activation unless final output\n",
    "            if out_channels != input_channels:\n",
    "                deconv_blocks.append(nn.ReLU())\n",
    "\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(*deconv_blocks)\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    #  Utility: convert label y → a one-hot feature map\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    def make_label_map(self, y, H, W):\n",
    "        \"\"\"\n",
    "        Convert labels (B,) into a spatial tensor (B, num_classes, H, W)\n",
    "        Each pixel has the same one-hot vector.\n",
    "        \"\"\"\n",
    "        y_onehot = F.one_hot(y, num_classes=self.num_classes).float()\n",
    "        return y_onehot[:, :, None, None].expand(-1, -1, H, W)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    #  ENCODER\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    def encode(self, x, y):\n",
    "        B, _, H, W = x.shape\n",
    "\n",
    "        y_map = self.make_label_map(y, H, W)  # (B, C_class, 28, 28)\n",
    "        inp = torch.cat([x, y_map], dim=1)\n",
    "\n",
    "        h = self.encoder_conv(inp)\n",
    "        h = self.pool(h)  # (B, C_last, 14, 14)\n",
    "\n",
    "        h = torch.flatten(h, start_dim=1)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    #  DECODER\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    def decode(self, z, y):\n",
    "        B = z.size(0)\n",
    "\n",
    "        y_onehot = F.one_hot(y, num_classes=self.num_classes).float()\n",
    "        latent_input = torch.cat([z, y_onehot], dim=1)\n",
    "\n",
    "        h = self.fc_decode(latent_input)\n",
    "        h = h.view(B, self.hidden_channels[-1], 14, 14)\n",
    "\n",
    "        x_hat = self.decoder_conv(h)\n",
    "        x_hat = torch.sigmoid(x_hat)\n",
    "        return x_hat\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    #  FORWARD\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mu, logvar = self.encode(x, y)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z, y)\n",
    "        return x_hat, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc753792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x_hat, x, mu, logvar):\n",
    "    B = x.size(0)\n",
    "    recon = F.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return (recon + kl) / B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cvae(model, dataloader, epochs=10, lr=1e-3, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_onehot = F.one_hot(y, num_classes=model.num_classes).float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if isinstance(model, CVAE_CNN):\n",
    "                x_hat, mu, logvar = model(x, y)\n",
    "            elif isinstance(model, CVAE_MLP):\n",
    "                x_hat, mu, logvar = model(x, y_onehot)\n",
    "            loss = vae_loss(x_hat, x, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | loss = {total_loss / len(dataloader.dataset):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dce9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_digit(model, n_samples=8, device=\"cpu\"):\n",
    "    all_imgs = []\n",
    "    model.eval()\n",
    "    for digit in range(10):\n",
    "        with torch.no_grad():\n",
    "            y = torch.full((n_samples,), digit, dtype=torch.long).to(device)\n",
    "            y_onehot = F.one_hot(y, num_classes=10).float()\n",
    "            z = torch.randn(n_samples, model.latent_dim).to(device)\n",
    "            if isinstance(model, CVAE_CNN):\n",
    "                imgs = model.decode(z, y).cpu()\n",
    "            elif isinstance(model, CVAE_MLP):\n",
    "                imgs = model.decode(z, y_onehot).cpu()\n",
    "            all_imgs.append(imgs)\n",
    "    \n",
    "    scale = 0.7\n",
    "    plt.figure(figsize=(n_samples*scale, 10*scale))\n",
    "    for digit in range(10):\n",
    "        imgs = all_imgs[digit]\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(10,n_samples,digit*n_samples+i+1)\n",
    "            plt.imshow(imgs[i].squeeze(), cmap=\"gray\")\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    return imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_mlp = CVAE_MLP(hidden_dims=[400, 100], latent_dim=20)\n",
    "train_cvae(cvae_mlp, train_loader, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6665c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs_mlp = generate_digit(cvae_mlp, n_samples=8, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a96818",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_cnn = CVAE_CNN(hidden_channels=[32, 64, 128], latent_dim=20)\n",
    "train_cvae(cvae_cnn, train_loader, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs_cnn = generate_digit(cvae_cnn, n_samples=8, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01ca77",
   "metadata": {},
   "source": [
    "## Text generation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9d17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pretrained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Example: mask a word\n",
    "text_examples = [\n",
    "    \"The capital of France is [MASK].\",\n",
    "    \"The largest planet in our solar system is [MASK].\",\n",
    "    \"The most passionate programming language is [MASK].\"\n",
    "]\n",
    "\n",
    "for text in text_examples:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "    # Get the token id with the highest probability at the masked position\n",
    "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "    predicted_token_id = predictions[0, mask_token_index, :].argmax(dim=-1)\n",
    "    \n",
    "    top_k = 5\n",
    "    top_k_token_ids = predictions[0, mask_token_index, :].topk(top_k).indices\n",
    "    top_k_token_probs = predictions[0, mask_token_index, :].topk(top_k).values\n",
    "    str_top_k = [f\"{tokenizer.decode([token_id])} ({prob.item():.1f})\" for token_id, prob in zip(top_k_token_ids[0], top_k_token_probs[0])]\n",
    "\n",
    "    predicted_token = tokenizer.decode(predicted_token_id)\n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"Original text: {text}\")\n",
    "    print(f\"Predicted token (top {top_k}): {str_top_k}\")\n",
    "    print(f\"Filled sentence: {text.replace(tokenizer.mask_token, predicted_token)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b434a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "gpt2_model.eval()\n",
    "\n",
    "# Example prompt\n",
    "prompt_examples = [\n",
    "    \"Once upon a time in a galaxy far, far away\",\n",
    "    \"In the future, artificial intelligence will\",\n",
    "    \"The secret to a happy life is\"\n",
    "]\n",
    "\n",
    "for prompt in prompt_examples:\n",
    "    inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output_ids = gpt2_model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "\n",
    "    generated_text = gpt2_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(\"Generated:\", generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
