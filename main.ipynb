{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6254286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from models import SimpleMLP, SimpleCNN, CVAE_MLP, CVAE_CNN\n",
    "\n",
    "from utils import experiment_classif_simple, format_results, \\\n",
    "    plot_loss_acc_over_epochs, plot_time_vs_parameters, \\\n",
    "    train_cvae, generate_digit\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# device = 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f1a5b",
   "metadata": {},
   "source": [
    "## Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ae0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = 1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0dec2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000000/10000000 [00:00<00:00, 14244445.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(10000000)):\n",
    "    _ = 1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bfbbdc",
   "metadata": {},
   "source": [
    "## Setup MNIST Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b5511",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ae02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Plot examples of digits (with true labels)\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "fig = plt.figure()\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow(example_data[i].cpu().squeeze(), cmap='gray')\n",
    "    plt.title(f\"True: {example_targets[i].item()}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f80a6",
   "metadata": {},
   "source": [
    "## Classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac19d8",
   "metadata": {},
   "source": [
    "### Expe - Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a82e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SimpleMLP with hidden dimensions: [32, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|█████| 10/10 [00:50<00:00,  5.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SimpleMLP with hidden dimensions: [64, 32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|█████| 10/10 [00:44<00:00,  4.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SimpleMLP with hidden dimensions: [128, 64, 32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|█████| 10/10 [00:51<00:00,  5.20s/it]\n"
     ]
    }
   ],
   "source": [
    "hidden_dims_tested = [\n",
    "    [32, 16], \n",
    "    [64, 32],\n",
    "    [128, 64, 32]\n",
    "]\n",
    "all_outputs_mlp = []\n",
    "all_models_mlp = []\n",
    "\n",
    "for hidden_dims in hidden_dims_tested:\n",
    "    print(f\"Training SimpleMLP with hidden dimensions: {hidden_dims}\")\n",
    "    model = SimpleMLP(hidden_dims=hidden_dims)\n",
    "    output = experiment_classif_simple(\n",
    "        SimpleMLP(hidden_dims=hidden_dims), \n",
    "        train_loader, test_loader,\n",
    "        nbr_epochs=10, device=device,\n",
    "        run_name=f\"SimpleMLP_{'_'.join(map(str, hidden_dims))}\"\n",
    "    )\n",
    "    all_outputs_mlp.append(output)\n",
    "    all_models_mlp.append(model)\n",
    "\n",
    "results_long_mlp, results_summary_mlp = format_results(all_outputs_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ae405",
   "metadata": {},
   "source": [
    "### Expe - Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54dc63cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SimpleCNN with hidden channels: [8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|█████| 10/10 [00:46<00:00,  4.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SimpleCNN with hidden channels: [16, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|█████| 10/10 [00:51<00:00,  5.10s/it]\n"
     ]
    }
   ],
   "source": [
    "hidden_channels_tested = [\n",
    "    [8], \n",
    "    [16, 16]\n",
    "]\n",
    "all_outputs_cnn = []\n",
    "all_models_cnn = []\n",
    "\n",
    "for hidden_chans in hidden_channels_tested:\n",
    "    print(f\"Training SimpleCNN with hidden channels: {hidden_chans}\")\n",
    "    model = SimpleCNN(hidden_channels=hidden_chans)\n",
    "    output = experiment_classif_simple(\n",
    "        model, \n",
    "        train_loader, test_loader,\n",
    "        nbr_epochs=10, device=device,\n",
    "        run_name=f\"SimpleCNN_{'_'.join(map(str, hidden_chans))}\"\n",
    "    )\n",
    "    all_outputs_cnn.append(output)\n",
    "    all_models_cnn.append(model)\n",
    "\n",
    "results_long_cnn, results_summary_cnn = format_results(all_outputs_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3799a",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1267aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_summary = pd.concat([results_summary_mlp, results_summary_cnn], ignore_index=True)\n",
    "results_long = pd.concat([results_long_mlp, results_long_cnn], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_vs_parameters(results_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2427f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc_over_epochs(results_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de75a8",
   "metadata": {},
   "source": [
    "## Image generation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798f7f3",
   "metadata": {},
   "source": [
    "### CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1732da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVAE_MLP number of parameters: 720024\n",
      "Epoch 1 | loss = 2.4360\n",
      "Epoch 2 | loss = 1.8473\n",
      "Epoch 3 | loss = 1.7306\n",
      "Epoch 4 | loss = 1.6750\n",
      "Epoch 5 | loss = 1.6410\n",
      "Epoch 6 | loss = 1.6187\n",
      "Epoch 7 | loss = 1.6012\n",
      "Epoch 8 | loss = 1.5875\n",
      "Epoch 9 | loss = 1.5765\n",
      "Epoch 10 | loss = 1.5675\n"
     ]
    }
   ],
   "source": [
    "cvae_mlp = CVAE_MLP(hidden_dims=[400, 100], latent_dim=20)\n",
    "\n",
    "num_params = sum(p.numel() for p in cvae_mlp.parameters() if p.requires_grad)\n",
    "print(f\"CVAE_MLP number of parameters: {num_params}\")\n",
    "\n",
    "train_cvae(cvae_mlp, train_loader, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6665c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs_mlp = generate_digit(cvae_mlp, n_samples=8, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5a96818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVAE_CNN number of parameters: 1969609\n",
      "Epoch 1 | loss = 1.9717\n",
      "Epoch 2 | loss = 1.5769\n",
      "Epoch 3 | loss = 1.5343\n",
      "Epoch 4 | loss = 1.5104\n",
      "Epoch 5 | loss = 1.4934\n"
     ]
    }
   ],
   "source": [
    "cvae_cnn = CVAE_CNN(hidden_channels=[32, 64, 128], latent_dim=20)\n",
    "\n",
    "num_params = sum(p.numel() for p in cvae_cnn.parameters() if p.requires_grad)\n",
    "print(f\"CVAE_CNN number of parameters: {num_params}\")\n",
    "\n",
    "train_cvae(cvae_cnn, train_loader, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs_cnn = generate_digit(cvae_cnn, n_samples=8, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01ca77",
   "metadata": {},
   "source": [
    "## Text generation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f9d17fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raphael_benerradi/Documents/Thèse/LIRMM/01-MCE/2025-2026 Polytech/Cours IA et environnement/AI-Impacts-lab_session/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT number of parameters: 109514298\n",
      "--------------------------------\n",
      "Original text: The capital of France is [MASK].\n",
      "Predicted token (top 5): ['paris (12.3)', 'lille (10.6)', 'lyon (10.5)', 'marseille (10.1)', 'tours (9.7)']\n",
      "Filled sentence: The capital of France is paris.\n",
      "--------------------------------\n",
      "Original text: The largest planet in our solar system is [MASK].\n",
      "Predicted token (top 5): ['earth (10.3)', 'pluto (10.2)', 'jupiter (9.9)', 'mars (9.9)', 'saturn (9.3)']\n",
      "Filled sentence: The largest planet in our solar system is earth.\n",
      "--------------------------------\n",
      "Original text: The most passionate programming language is [MASK].\n",
      "Predicted token (top 5): ['java (10.5)', 'python (9.9)', 'c (9.8)', 'english (9.0)', 'php (7.4)']\n",
      "Filled sentence: The most passionate programming language is java.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pretrained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"BERT number of parameters: {num_params}\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Example: mask a word\n",
    "text_examples = [\n",
    "    \"The capital of France is [MASK].\",\n",
    "    \"The largest planet in our solar system is [MASK].\",\n",
    "    \"The most passionate programming language is [MASK].\"\n",
    "]\n",
    "\n",
    "for text in text_examples:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "    # Get the token id with the highest probability at the masked position\n",
    "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "    predicted_token_id = predictions[0, mask_token_index, :].argmax(dim=-1)\n",
    "    \n",
    "    top_k = 5\n",
    "    top_k_token_ids = predictions[0, mask_token_index, :].topk(top_k).indices\n",
    "    top_k_token_probs = predictions[0, mask_token_index, :].topk(top_k).values\n",
    "    str_top_k = [f\"{tokenizer.decode([token_id])} ({prob.item():.1f})\" for token_id, prob in zip(top_k_token_ids[0], top_k_token_probs[0])]\n",
    "\n",
    "    predicted_token = tokenizer.decode(predicted_token_id)\n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"Original text: {text}\")\n",
    "    print(f\"Predicted token (top {top_k}): {str_top_k}\")\n",
    "    print(f\"Filled sentence: {text.replace(tokenizer.mask_token, predicted_token)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6b434a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 number of parameters: 124439808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Prompt: Once upon a time in a galaxy far, far away\n",
      "Generated: Once upon a time in a galaxy far, far away, there was a galaxy with a vast number of planets. These celestial objects formed when light and heat were created. They created the universe in such a way that our world may become completely uninhabitable. They are also known as \"supernovas.\"\n",
      "\n",
      "Supernovas are the first supermassive black holes in the observable Universe that have the ability to grow at incredible speeds in a matter of minutes. The expansion of the black hole is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Prompt: In the future, artificial intelligence will\n",
      "Generated: In the future, artificial intelligence will be the most important technology that could change the way we work, play, and travel our daily lives. It will bring a lot of benefits to our society as a whole, and will make it possible for everyone. So, the future is bright!\"\n",
      "\n",
      "The report also states that artificial intelligence can take jobs from humans and robots to new heights.\n",
      "\n",
      "Dr. David Wieber, the chief executive of the University of Wisconsin, wrote, \"By the\n",
      "--------------------------------\n",
      "Prompt: The secret to a happy life is\n",
      "Generated: The secret to a happy life is never to disappoint.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in gpt2_model.parameters() if p.requires_grad)\n",
    "print(f\"GPT-2 number of parameters: {num_params}\")\n",
    "\n",
    "gpt2_model.eval()\n",
    "\n",
    "# Example prompt\n",
    "prompt_examples = [\n",
    "    \"Once upon a time in a galaxy far, far away\",\n",
    "    \"In the future, artificial intelligence will\",\n",
    "    \"The secret to a happy life is\"\n",
    "]\n",
    "\n",
    "for prompt in prompt_examples:\n",
    "    inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output_ids = gpt2_model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "\n",
    "    generated_text = gpt2_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(\"Generated:\", generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
